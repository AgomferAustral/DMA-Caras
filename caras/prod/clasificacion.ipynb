{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZSWwCvVjcEM"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yo5z0DKSjI0j"
      },
      "source": [
        "Carga librerias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfijvzlBjK_k"
      },
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from sklearn.manifold import Isomap\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXOtbhFfnAF1"
      },
      "source": [
        "# Clase Multiperceptron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_WjG9TwmyEz"
      },
      "outputs": [],
      "source": [
        "# definicion de las funciones de activacion\n",
        "# y sus derivadas agregando las versiones VECTORIZADAS\n",
        "def func_eval(fname, x):\n",
        "    if fname == \"purelin\":\n",
        "        return x\n",
        "    elif fname == \"logsig\":\n",
        "        return 1.0 / (1.0 + np.exp(-x))\n",
        "    elif fname == \"tansig\":\n",
        "        return 2.0 / (1.0 + np.exp(-2.0 * x)) - 1.0\n",
        "    else:\n",
        "        raise ValueError(f\"Función de activación no soportada: {fname}\")\n",
        "\n",
        "# version vectorizada de func_eval\n",
        "func_eval_vec = np.vectorize(func_eval)\n",
        "\n",
        "def deriv_eval(fname, y):\n",
        "    if fname == \"purelin\":\n",
        "        return 1.0\n",
        "    elif fname == \"logsig\":\n",
        "        return y * (1.0 - y)\n",
        "    elif fname == \"tansig\":\n",
        "        return 1.0 - y * y\n",
        "\n",
        "# version vectorizada de deriv_eval\n",
        "deriv_eval_vec = np.vectorize(deriv_eval)\n",
        "\n",
        "# Definicion de la clase de multiperceptron (red neuronal con múltiples capas ocultas)\n",
        "\n",
        "class multiperceptron(object):\n",
        "    \"\"\"Multiperceptron class\"\"\"\n",
        "\n",
        "    def _red_init(self, semilla) -> None:\n",
        "\n",
        "        niveles = self.red['arq']['layers_qty']\n",
        "        np.random.seed(semilla)\n",
        "\n",
        "        for i in range(niveles):\n",
        "           nivel = dict()\n",
        "           nivel['id'] = i\n",
        "           nivel['last'] = (i==(niveles-1))\n",
        "           nivel['size'] = self.red[\"arq\"][\"layers_size\"][i]\n",
        "           nivel['func'] = self.red[\"arq\"][\"layers_func\"][i]\n",
        "\n",
        "           # Se determina cuántas entradas recibe la capa actual\n",
        "           if( i==0 ):\n",
        "              entrada_size = self.red['arq']['input_size']\n",
        "           else:\n",
        "              entrada_size =  self.red['arq']['layers_size'][i-1]\n",
        "\n",
        "           salida_size =  nivel['size']\n",
        "\n",
        "           # los pesos, inicializados random\n",
        "           nivel['W'] = np.random.uniform(-0.5, 0.5, [salida_size, entrada_size])\n",
        "           nivel['w0'] = np.random.uniform(-0.5, 0.5, [salida_size, 1])\n",
        "\n",
        "           # los momentos, inicializados en CERO\n",
        "           nivel['W_m'] = np.zeros([salida_size, entrada_size])\n",
        "           nivel['w0_m'] = np.zeros([salida_size, 1])\n",
        "\n",
        "           self.red['layer'].append(nivel)\n",
        "\n",
        "    # constructor generico\n",
        "    def __init__(self) -> None:\n",
        "        self.data = dict()\n",
        "        self.red = dict()\n",
        "        self.carpeta = \"\"\n",
        "\n",
        "    # inicializacion full\n",
        "    def inicializar(self, df, campos, clase, hidden_layers_sizes, layers_func,\n",
        "                 semilla, carpeta) -> None:\n",
        "\n",
        "        # Procesamiento de los datos de entrada (features)\n",
        "        self.data['X'] = np.array(df.select(campos))\n",
        "        # Normalización (escalado) de los datos para que los atributos tengan media 0 y desviación estándar 1\n",
        "        X_mean = self.data['X'].mean(axis=0)\n",
        "        X_sd = self.data['X'].std(axis=0)\n",
        "        self.data['X'] = (self.data['X'] - X_mean)/X_sd\n",
        "\n",
        "        #  Procesamiento de etiquetas Ylabel en  numpy\n",
        "        label =df.select(clase)\n",
        "        self.data['Ylabel'] = np.array(label).reshape(len(label))\n",
        "\n",
        "        # one-hot-encoding de Y . Realiza la codificacion \"one hot\" de las etiquetas (Convierte cada categoría única en una columna binaria separada y representa la presencia con 1 y la ausencia con 0.)\n",
        "        col_originales = df.columns\n",
        "        self.data['Y'] = np.array( df.to_dummies(clase).drop(col_originales, strict=False) )\n",
        "        col_dummies = sorted( list( set(df.to_dummies(clase).columns) -  set(col_originales)))\n",
        "        # Need to import reduce from functools\n",
        "        from functools import reduce\n",
        "        clases_originales = reduce(lambda acc, x: acc + [x[(len(clase)+1):]], col_dummies, [])\n",
        "\n",
        "\n",
        "        # Construcción de la arquitectura de la red\n",
        "        tamanos = hidden_layers_sizes\n",
        "        tamanos.append(self.data['Y'].shape[1])\n",
        "\n",
        "        arquitectura = {\n",
        "             'input_size' : self.data['X'].shape[1],\n",
        "             'input_mean' : X_mean,\n",
        "             'input_sd' :  X_sd,\n",
        "             'output_values' : clases_originales,\n",
        "             'layers_qty' : len(hidden_layers_sizes),\n",
        "             'layers_size' : tamanos ,\n",
        "             'layers_func' : layers_func,\n",
        "        }\n",
        "\n",
        "        self.red['arq'] = arquitectura\n",
        "\n",
        "        # inicializo  work . Inicialización de información de entrenamiento\n",
        "        self.red['work'] = dict()\n",
        "        self.red['work']['epoch'] = 0\n",
        "        self.red['work']['MSE'] = float('inf')\n",
        "        self.red['work']['train_error_rate'] = float('inf')\n",
        "\n",
        "        # Preparación de las capas y pesos\n",
        "        self.red['layer'] = list()\n",
        "        self._red_init(semilla)\n",
        "\n",
        "        # grabo el entorno. Guardar el entorno en disco\n",
        "        self.carpeta = carpeta\n",
        "        os.makedirs(self.carpeta, exist_ok=True)\n",
        "        with open(self.carpeta+\"/data.pkl\", 'wb') as f:\n",
        "            pickle.dump(self.data, f)\n",
        "\n",
        "        with open(self.carpeta+\"/red.pkl\", 'wb') as f:\n",
        "            pickle.dump(self.red, f)\n",
        "\n",
        "\n",
        "    # predigo a partir de modelo recien entrenado\n",
        "    def  predecir(self, df_new, campos, clase) -> None:\n",
        "\n",
        "\n",
        "        niveles = self.red['arq']['layers_qty']\n",
        "\n",
        "        # etapa forward\n",
        "        # recorro hacia adelante, nivel a nivel\n",
        "        X_new =  np.array( df_new.select(campos))\n",
        "\n",
        "\n",
        "        # estandarizo manualmente con las medias y desvios que almacene durante el entrenamiento\n",
        "        X_new = (X_new - self.red['arq']['input_mean'])/self.red['arq']['input_sd']\n",
        "\n",
        "\n",
        "        # la entrada a la red,  el X que es TODO  x_new\n",
        "        entrada = X_new.T\n",
        "\n",
        "        for i in range(niveles):\n",
        "          estimulos = self.red['layer'][i]['W'] @ entrada + self.red['layer'][i]['w0']\n",
        "          salida =  func_eval_vec(self.red['layer'][i]['func'], estimulos)\n",
        "          entrada = salida  # para la proxima vuelta\n",
        "\n",
        "        # me quedo con la neurona de la ultima capa que se activio con mayor intensidad\n",
        "        pred_idx = np.argmax( salida.T, axis=1)\n",
        "        pred_raw = np.max( salida.T, axis=1)\n",
        "        # Inicializamos out y error_rate a None\n",
        "        out=None\n",
        "        new_error_rate = None\n",
        "\n",
        "        if clase: # Verificamos si la variable clase tiene un valor (no es cadena vacía o None)\n",
        "            out = np.array(self.red['arq']['output_values'])\n",
        "\n",
        "            true_labels = np.array(df_new.select(clase)).flatten() # Aseguramos que sea un array plano\n",
        "\n",
        "            predicted_labels = out[pred_idx] # Estos serán strings (nombres de clase)\n",
        "            new_error_rate = np.mean(true_labels != predicted_labels)\n",
        "\n",
        "        # Modify the return statement to handle the case where 'out' is None\n",
        "        # If 'out' is None, we return the raw indices instead of trying to use 'out'\n",
        "        # If 'out' is not None, we return the predicted class names based on the indices\n",
        "        if out is not None:\n",
        "            return (out[pred_idx], pred_raw, new_error_rate)\n",
        "        else:\n",
        "            # When 'clase' is not provided, we cannot return class names, so we return\n",
        "            # the indices of the activated neurons instead of trying to access 'out'\n",
        "            # We also return None for new_error_rate as true labels are not available\n",
        "            return (pred_idx, pred_raw, new_error_rate)\n",
        "\n",
        "\n",
        "    # cargo un modelo ya entrenado, grabado en carpeta\n",
        "    def cargar_modelo(self, carpeta) -> None:\n",
        "        self.carpeta = carpeta\n",
        "\n",
        "        with open(self.carpeta+\"/red.pkl\", 'rb') as f:\n",
        "          self.red = pickle.load(f)\n",
        "\n",
        "        return (self.red['work']['epoch'],\n",
        "                self.red['work']['MSE'],\n",
        "                self.red['work']['train_error_rate'] )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8jLPcUudOP0"
      },
      "source": [
        "# Directorios y campos de entrada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWK-9_jWdVIR"
      },
      "outputs": [],
      "source": [
        "modelo_path = \"/content/drive/MyDrive/caras/prod\"\n",
        "datos_path = \"/content/drive/MyDrive/caras/prod\"\n",
        "imagenes_path =\"/content/drive/MyDrive/Detectadasprod21\" # Definido en procesamientoprod.ipynb\n",
        "salida_path = \"/content/drive/MyDrive/caras/prod/resultados.csv\"\n",
        "\n",
        "campos = [f\"PC{i}\" for i in range(1, 51)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lM02moEnS89"
      },
      "source": [
        "# Ejecución"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyI4XLrZqThu"
      },
      "source": [
        "Carga de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGt71d0CCkqp"
      },
      "outputs": [],
      "source": [
        "imagenes = []\n",
        "etiquetas = []\n",
        "\n",
        "print(\"Cargando imágenes...\")\n",
        "for persona in os.listdir(imagenes_path):\n",
        "    ruta_persona = os.path.join(imagenes_path, persona)\n",
        "    if os.path.isdir(ruta_persona):\n",
        "        for img in os.listdir(ruta_persona):\n",
        "            if img.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                ruta_imagen = os.path.join(ruta_persona, img)\n",
        "                imagenes.append(ruta_imagen)\n",
        "                etiquetas.append(persona)\n",
        "\n",
        "print(f\"Total de imágenes cargadas: {len(imagenes)}\")\n",
        "if imagenes:\n",
        "    muestra = np.array(Image.open(imagenes[0]).convert('L'))\n",
        "    print(f\"Dimensión de cada imagen: {muestra.shape} (total features: {muestra.size})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-q9fnZt-HbE7"
      },
      "outputs": [],
      "source": [
        "# Identificar archivos de imagenes sin etiquetas\n",
        "def leer_imagenes(i_path):\n",
        "    \"\"\"\n",
        "    Identifica archivos de imagen en una carpeta que no están en subcarpetas (consideradas \"etiquetas\").\n",
        "    Args:\n",
        "        i_path (str): La ruta a la carpeta principal donde buscar imágenes.\n",
        "    Returns:\n",
        "        list: Una lista de rutas a archivos de imagen que no se encuentran en ninguna subcarpeta.\n",
        "    \"\"\"\n",
        "    imagenes = []\n",
        "    if not os.path.isdir(i_path):\n",
        "        print(f\"La ruta '{i_path}' no es un directorio válido.\")\n",
        "        return imagenes\n",
        "\n",
        "    for item in os.listdir(i_path):\n",
        "        item_path = os.path.join(i_path, item)\n",
        "        if os.path.isfile(item_path) and item.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            imagenes.append(item_path)\n",
        "\n",
        "    return imagenes\n",
        "\n",
        "print(f\"Buscando imágenes sin etiqueta en: {imagenes_path}\")\n",
        "imagenes = leer_imagenes(imagenes_path)\n",
        "\n",
        "if imagenes:\n",
        "    print(f\"Se encontraron {len(imagenes)} imágenes sin etiqueta:\")\n",
        "else:\n",
        "    print(\"No se encontraron imágenes sin etiqueta en el directorio especificado.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xowSlP80DHrQ"
      },
      "outputs": [],
      "source": [
        "print(\"Vectorizando imágenes y aplicando reducción de dimensionalidad con Isomap...\")\n",
        "imagenes_vectorizadas = np.array([np.array(Image.open(img).convert('L')).flatten() for img in imagenes])\n",
        "\n",
        "isomap = Isomap(n_neighbors=14, n_components=50)\n",
        "imagenes_isomap = isomap.fit_transform(imagenes_vectorizadas)\n",
        "\n",
        "print(f\"Dimensión después de Isomap: {imagenes_isomap.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D74e3X3mqTJc"
      },
      "outputs": [],
      "source": [
        "data_dict = {f\"PC{i+1}\": imagenes_isomap[:, i] for i in range(imagenes_isomap.shape[1])}\n",
        "\n",
        "df_isomap = pl.DataFrame(data_dict)\n",
        "# El DataFrame df_isomap ahora contiene los datos de imagenes_isomap\n",
        "# en un formato similar a un CSV tab-separado, incluyendo la ruta original de la imagen.\n",
        "\n",
        "print(\"DataFrame creado a partir de Isomap:\")\n",
        "print(df_isomap.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgavcyR3qShX"
      },
      "source": [
        "Carga modelo entrenado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TM_AtKD4qotm"
      },
      "outputs": [],
      "source": [
        "mp_prod = multiperceptron()\n",
        "mp_prod.cargar_modelo(modelo_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bsh5J9kjKX6"
      },
      "source": [
        "Prediccion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdUAMvn6jJ-5"
      },
      "outputs": [],
      "source": [
        "# Predecir (pasamos una columna vacía como clase)\n",
        "\n",
        "(y_hat, y_raw, new_error_rate)= mp_prod.predecir(df_isomap, campos, clase=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0Yxo9AubP0a"
      },
      "source": [
        "Visualizacion de las Predicciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uo4jeFXh65Q4"
      },
      "outputs": [],
      "source": [
        "#visualizar las predicciones con el nombre de las etiquetas usadas para entrenamiento\n",
        "\n",
        "with open(modelo_path + \"/red.pkl\", 'rb') as f:# Cargar el archivo red.pkl para obtener los nombres de las etiquetas\n",
        "    red_info = pickle.load(f)\n",
        "\n",
        "clases_originales = red_info['arq']['output_values']# Obtener los nombres de las etiquetas originales\n",
        "\n",
        "# Crea un nuevo DataFrame Polars con los resultados\n",
        "df_resultados = df_isomap.with_columns([\n",
        "    pl.Series(name=\"etiqueta_predicha\", values=[clases_originales[idx] for idx in y_hat]),\n",
        "    pl.Series(name=\"intensidad_predicha\", values=y_raw)\n",
        "])\n",
        "\n",
        "# Guarda el nuevo DataFrame en un archivo CSV\n",
        "df_resultados.write_csv(salida_path, separator='\\t')\n",
        "print(f\"Resultados guardados en: {salida_path}\")\n",
        "\n",
        "# Mostrar el DataFrame de resultados (opcional)\n",
        "print(\"\\nDataFrame con predicciones y etiquetas:\")\n",
        "print(df_resultados.head())\n",
        "\n",
        "# Crea una lista de descripciones para cada predicción\n",
        "descripciones = [f\"Muestra {i}\" for i in range(len(y_hat))]\n",
        "\n",
        "# Crea un DataFrame Polars con los resultados formateados\n",
        "df_prediccion_formateada = pl.DataFrame({\n",
        "    \"Descripción\": descripciones,\n",
        "    \"Etiqueta Predicha\": [clases_originales[idx] for idx in y_hat],\n",
        "    \"Intensidad Predicha\": y_raw\n",
        "}).with_columns(\n",
        "    # Redondear la columna \"Intensidad Predicha\" a 5 decimales\n",
        "    pl.col(\"Intensidad Predicha\").round(5)\n",
        ")\n",
        "\n",
        "# Imprime el DataFrame\n",
        "print(\"\\nResultados de la predicción en formato tabla:\")\n",
        "df_prediccion_formateada\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHMpU-qjeDGT"
      },
      "source": [
        "#  Definicion de umbral de reconocimiento (Deteccion de Itrusos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRaH5M68ePlA"
      },
      "outputs": [],
      "source": [
        "# Filtrar las predicciones basándose en el umbral de confianza y asignar\n",
        "umbral_confianza = 0.5 # Umbral de confianza\n",
        "\n",
        "\n",
        "df_sin_reconocer = df_resultados.filter(pl.col(\"intensidad_predicha\") < umbral_confianza)\n",
        "\n",
        "# Mostrar las filas que corresponden a sin reconocer\n",
        "print(f\"\\nRegistros sin identificar (intensidad < {umbral_confianza}):\")\n",
        "if df_sin_reconocer.height > 0:\n",
        "    print(df_sin_reconocer)\n",
        "else:\n",
        "    print(\"Se identificaron todos los registros dentro del umbral especificado.\")\n",
        "\n",
        "# Mostrar o procesar las imágenes no identificadas\n",
        "\n",
        "if not df_sin_reconocer.is_empty():\n",
        "    print(\"\\nVisualizando las primeras 5 imágenes sin reconocer:\")\n",
        "    sin_reconocer_paths = df_sin_reconocer.select(\"ruta_imagen\").to_numpy().flatten()\n",
        "\n",
        "    # We need the original indices from the 'imagenes' list to access the images correctly\n",
        "    original_indices = [imagenes.index(path) for path in sin_reconocer_paths]\n",
        "\n",
        "\n",
        "    # Visualizar las imágenes correspondientes to those indices (using the original 'imagenes' list)\n",
        "    num_to_show = min(5, len(original_indices))\n",
        "    fig, axes = plt.subplots(1, num_to_show, figsize=(20, 5))\n",
        "\n",
        "    if num_to_show == 1:\n",
        "        axes = [axes] # Ensure axes is an array even for a single image\n",
        "\n",
        "    for i in range(num_to_show):\n",
        "        img_index_original = original_indices[i]\n",
        "        img_path = imagenes[img_index_original] # Using the original list of paths\n",
        "        current_sin_reconocer_row = df_sin_reconocer.filter(pl.col(\"ruta_imagen\") == img_path)\n",
        "        current_intensity = current_sin_reconocer_row[\"intensidad_predicha\"][0]\n",
        "\n",
        "        try:\n",
        "            img = mpimg.imread(img_path)\n",
        "            axes[i].imshow(img, cmap='gray') # assuming they are grayscale images\n",
        "            axes[i].set_title(f\"Sin Reconocer {i+1}\\nIntensidad: {current_intensity:.4f}\")\n",
        "            axes[i].axis('off')\n",
        "        except FileNotFoundError:\n",
        "            axes[i].set_title(f\"Error loading\\n{os.path.basename(img_path)}\")\n",
        "            axes[i].axis('off')\n",
        "            print(f\"Error loading image for visualization {img_path}: File not found\")\n",
        "        except Exception as e:\n",
        "            axes[i].set_title(f\"Error loading\\n{os.path.basename(img_path)}\")\n",
        "            axes[i].axis('off')\n",
        "            print(f\"Error loading image for visualization {img_path}: {e}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Sin imagenes sin reconocer dentro del umbral para mostrar.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D96hCztJ9u4W"
      },
      "source": [
        "# Visualizacion de la imagenes con la prediccion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7q4SuxQZg3pT"
      },
      "outputs": [],
      "source": [
        "# Filtra los resultados en base al umbral de confianza para reconomiento\n",
        "df_reconocidas = df_resultados.filter(pl.col(\"intensidad_predicha\") >= umbral_confianza)\n",
        "\n",
        "# Cuenta la cantidad de imagenes sin reconocer\n",
        "num_sin_reconocer = df_sin_reconocer.height\n",
        "\n",
        "print(f\"\\nCantidad de imágenes sin reconocer (Intensidad < {umbral_confianza}): {num_sin_reconocer}\")\n",
        "\n",
        "# Visualización de las imágenes reconocidas por encima del umbral\n",
        "if not df_reconocidas.is_empty():\n",
        "    print(f\"\\nMostrando imágenes reconocidas (Intensidad >= {umbral_confianza}):\")\n",
        "    for row in df_reconocidas.iter_rows(named=True):\n",
        "        ruta_imagen = row[\"ruta_imagen\"]\n",
        "        prediccion = row[\"etiqueta_predicha\"]\n",
        "        intensidad = row[\"intensidad_predicha\"]\n",
        "        try:\n",
        "            img = mpimg.imread(ruta_imagen)\n",
        "            plt.figure(figsize=(3, 3))\n",
        "            plt.imshow(img, cmap='gray') # Usar cmap='gray' si son imágenes en escala de grises\n",
        "            plt.title(f\"Predicción: {prediccion} ({intensidad:.2f})\")\n",
        "            plt.axis('off') # Ocultar ejes\n",
        "            plt.show()\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: No se encontró la imagen en {ruta_imagen}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error al mostrar la imagen {ruta_imagen}: {e}\")\n",
        "    print(\"Visualización de imágenes reconocidas completada.\")\n",
        "else:\n",
        "    print(\"No hay imágenes reconocidas por encima del umbral especificado para mostrar.\")\n",
        "\n",
        "# Visualización de las imágenes sin reconocer (las que están en df_sin_reconocer)\n",
        "if not df_sin_reconocer.is_empty():\n",
        "    print(f\"\\nMostrando imágenes sin reconocer (Intensidad < {umbral_confianza}):\")\n",
        "    for row in df_sin_reconocer.iter_rows(named=True):\n",
        "        ruta_imagen = row[\"ruta_imagen\"]\n",
        "        intensidad = row[\"intensidad_predicha\"]\n",
        "        try:\n",
        "            img = mpimg.imread(ruta_imagen)\n",
        "            plt.figure(figsize=(3, 3))\n",
        "            plt.imshow(img, cmap='gray') # Usar cmap='gray' si son imágenes en escala de grises\n",
        "            plt.title(f\"Sin Reconocer ({intensidad:.2f})\")\n",
        "            plt.axis('off') # Ocultar ejes\n",
        "            plt.show()\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: No se encontró la imagen en {ruta_imagen}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error al mostrar la imagen {ruta_imagen}: {e}\")\n",
        "    print(\"Visualización de imágenes sin reconocer completada.\")\n",
        "else:\n",
        "    print(\"No hay imágenes sin reconocer por debajo del umbral especificado para mostrar.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "QXOtbhFfnAF1",
        "3lM02moEnS89",
        "rHMpU-qjeDGT"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
