{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b98fdb60",
   "metadata": {},
   "source": [
    "\n",
    "# Reconocimiento Facial con Isomap y Perceptrón Multicapa (MLP)\n",
    "\n",
    "Este notebook implementa un pipeline completo de reconocimiento facial usando reducción de dimensionalidad con Isomap, clasificación con redes neuronales (MLP) y optimización de hiperparámetros con GridSearchCV. Se trabaja con imágenes originales y aumentadas organizadas por sujeto en carpetas individuales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36039880",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ccbcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "base_path = '/content/drive/MyDrive/proyecto_isomap'\n",
    "data_path = os.path.join(base_path, 'data')\n",
    "fotos_path = os.path.join(data_path, 'fotos')\n",
    "aumentadas_path = os.path.join(data_path, 'aumentadas')\n",
    "processed_path = os.path.join(base_path, 'processed')\n",
    "models_path = os.path.join(base_path, 'models')\n",
    "results_path = os.path.join(base_path, 'results')\n",
    "\n",
    "for path in [processed_path, models_path, results_path]:\n",
    "    os.makedirs(path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb08d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install joblib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2343ba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "img_size = (64, 64)\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    data = []\n",
    "    labels = []\n",
    "    label_names = sorted(os.listdir(folder))\n",
    "    for label in label_names:\n",
    "        label_path = os.path.join(folder, label)\n",
    "        if not os.path.isdir(label_path):\n",
    "            continue\n",
    "        for file in os.listdir(label_path):\n",
    "            try:\n",
    "                img_path = os.path.join(label_path, file)\n",
    "                img = Image.open(img_path).convert(\"L\").resize(img_size)\n",
    "                data.append(np.asarray(img).flatten())\n",
    "                labels.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"Error con {img_path}: {e}\")\n",
    "    return np.array(data), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da178a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import joblib\n",
    "\n",
    "fotos_file = os.path.join(processed_path, 'X_fotos.joblib')\n",
    "aug_file = os.path.join(processed_path, 'X_aug.joblib')\n",
    "\n",
    "if os.path.exists(fotos_file) and os.path.exists(aug_file):\n",
    "    X_fotos, y_fotos = joblib.load(fotos_file)\n",
    "    X_aug, y_aug = joblib.load(aug_file)\n",
    "else:\n",
    "    X_fotos, y_fotos = load_images_from_folder(fotos_path)\n",
    "    X_aug, y_aug = load_images_from_folder(aumentadas_path)\n",
    "    joblib.dump((X_fotos, y_fotos), fotos_file)\n",
    "    joblib.dump((X_aug, y_aug), aug_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bec2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_fotos_train, X_test, y_fotos_train, y_test = train_test_split(\n",
    "    X_fotos, y_fotos, test_size=0.3, stratify=y_fotos, random_state=42\n",
    ")\n",
    "\n",
    "X_train = np.concatenate([X_fotos_train, X_aug], axis=0)\n",
    "y_train = np.concatenate([y_fotos_train, y_aug], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dc4a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('iso', Isomap()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('mlp', MLPClassifier(max_iter=500, random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'iso__n_neighbors': [5, 10],\n",
    "    'iso__n_components': [20, 40],\n",
    "    'mlp__hidden_layer_sizes': [(100,), (50, 50)],\n",
    "    'mlp__learning_rate_init': [0.001, 0.01],\n",
    "    'mlp__activation': ['relu', 'tanh'],\n",
    "    'mlp__alpha': [0.0001, 0.001],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=3, scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bb2977",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = grid.predict(X_test)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Mejores hiperparámetros:\", grid.best_params_)\n",
    "print(\"\\nReporte de clasificación:\\n\", report)\n",
    "\n",
    "joblib.dump(grid.best_estimator_, os.path.join(models_path, 'mejor_modelo.joblib'))\n",
    "\n",
    "with open(os.path.join(results_path, 'clasificacion.txt'), 'w') as f:\n",
    "    f.write(\"Mejores hiperparámetros:\\n\")\n",
    "    f.write(str(grid.best_params_))\n",
    "    f.write(\"\\n\\nReporte de clasificación:\\n\")\n",
    "    f.write(report)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}